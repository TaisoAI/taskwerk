# Taskwerk v0.6.x AI Implementation Plan

## Overview
This plan outlines the implementation of AI features for Taskwerk v0.6.x, focusing on LLM plumbing, configuration, and core AI commands.

## Phase 1: LLM Infrastructure & Configuration (TASK-020)

### 1.1 Provider System Architecture
```
src/
├── ai/
│   ├── providers/
│   │   ├── base-provider.js      # Abstract base class
│   │   ├── anthropic.js          # Claude models
│   │   ├── openai.js             # GPT models
│   │   ├── bedrock.js            # AWS Bedrock
│   │   ├── grok.js               # Grok/X.AI
│   │   ├── mistral.js            # Mistral models
│   │   ├── ollama.js             # Local Ollama
│   │   └── lmstudio.js           # Local LM Studio
│   ├── llm-manager.js            # Main LLM orchestration
│   ├── model-registry.js         # Available models per provider
│   └── config-schema.js          # AI configuration schema
```

### 1.2 Configuration Command (`taskwerk aiconfig`)
- **Add provider keys**: `taskwerk aiconfig --set provider.key "value"`
- **List providers**: `taskwerk aiconfig --list-providers`
- **Choose model**: `taskwerk aiconfig --choose` (interactive model selection)
- **Test connection**: `taskwerk aiconfig --test`
- **Show current**: `taskwerk aiconfig --show`

### 1.3 Configuration Storage
```javascript
// In ~/.taskwerk/config.json
{
  "ai": {
    "current_provider": "anthropic",
    "current_model": "claude-3-opus-20240229",
    "providers": {
      "anthropic": {
        "api_key": "sk-ant-...",
        "enabled": true
      },
      "openai": {
        "api_key": "sk-...",
        "enabled": true,
        "base_url": "https://api.openai.com/v1"  // Optional for custom endpoints
      },
      "ollama": {
        "enabled": true,
        "base_url": "http://localhost:11434"
      },
      "bedrock": {
        "enabled": true,
        "region": "us-east-1",
        "access_key_id": "...",
        "secret_access_key": "..."
      }
    },
    "defaults": {
      "temperature": 0.7,
      "max_tokens": 2000
    }
  }
}
```

### 1.4 Model Discovery
- Auto-discover available models from each configured provider
- Cache model lists with periodic refresh
- Validate model availability before use

## Phase 2: Raw LLM Command (TASK-021)

### 2.1 Command: `taskwerk llm` (or `taskwerk ai`)
```bash
# Basic usage
taskwerk llm "What is the meaning of life?"

# With parameters
taskwerk llm "Explain {topic}" --params topic="quantum computing"

# Override provider/model
taskwerk llm "Hello" --provider openai --model gpt-4

# System prompt
taskwerk llm "Review this code" --system "You are a code reviewer"

# From file
taskwerk llm --file prompt.txt

# With context
taskwerk llm "Summarize these tasks" --context-tasks
```

### 2.2 Features
- Direct LLM interaction without task context
- Parameter substitution with `{}` placeholders
- Provider/model override (not persisted)
- Streaming response support
- Token usage reporting
- Response caching (optional)

### 2.3 Implementation
```javascript
// src/commands/llm.js
export function llmCommand() {
  const llm = new Command('llm');
  
  llm
    .description('Send a prompt directly to the configured LLM')
    .argument('[prompt]', 'The prompt to send')
    .option('-f, --file <path>', 'Read prompt from file')
    .option('-p, --params <key=value...>', 'Parameters for prompt substitution')
    .option('--provider <name>', 'Override provider for this request')
    .option('--model <name>', 'Override model for this request')
    .option('-s, --system <prompt>', 'System prompt')
    .option('--temperature <num>', 'Override temperature (0-2)')
    .option('--max-tokens <num>', 'Override max tokens')
    .option('--context-tasks', 'Include current tasks as context')
    .option('--no-stream', 'Disable streaming output')
    .action(async (prompt, options) => {
      // Implementation
    });
    
  return llm;
}
```

## Phase 3: AI Commands (TASK-018, TASK-019)

### 3.1 Ask Command
```bash
# Task-aware queries
taskwerk ask "What tasks are blocked?"
taskwerk ask "Summarize this week's progress"
taskwerk ask "Which tasks have no assignee?"

# Analysis queries
taskwerk ask "What's the critical path for PROJECT-001?"
taskwerk ask "Estimate completion time for all in-progress tasks"
```

### 3.2 Agent Mode (Future)
```bash
# Interactive planning
taskwerk agent plan "Implement user authentication"

# Task execution assistance
taskwerk agent work TASK-001

# Review mode
taskwerk agent review --branch feature/auth
```

## Implementation Priority

1. **TASK-020: AI Configuration** (High Priority)
   - Provider abstraction layer
   - Configuration management
   - Model discovery and validation
   - Connection testing

2. **TASK-021: Raw LLM Command** (High Priority)
   - Basic prompt execution
   - Parameter substitution
   - Streaming responses
   - Provider override

3. **TASK-018: Ask Command** (Medium Priority)
   - Task context integration
   - Query templates
   - Response formatting

4. **TASK-019: Agent Mode** (Medium Priority)
   - Interactive sessions
   - Context management
   - Safety guards

## Key Design Decisions

### 1. Provider Abstraction
- All providers implement a common interface
- Easy to add new providers
- Graceful fallback handling

### 2. Configuration Philosophy
- API keys stored in user config, not project
- Per-project model preferences possible
- Environment variable override support

### 3. Safety First
- Read-only operations in v0.6.x
- Clear user consent for any modifications
- Token usage tracking and limits

### 4. Context Management
- Efficient task context building
- Relevant information selection
- Privacy-aware (no sensitive data in prompts)

## Testing Strategy

### 1. Unit Tests
- Mock provider responses
- Configuration management
- Parameter substitution
- Error handling

### 2. Integration Tests
- Real provider connections (with test keys)
- Model discovery
- Streaming responses
- Context building

### 3. E2E Tests
- Full command flows
- Multi-provider scenarios
- Error recovery

## Success Criteria

1. **Flexible Provider Support**: Easy to configure and switch between providers
2. **Good Developer Experience**: Intuitive commands, helpful error messages
3. **Performance**: Fast response times, efficient token usage
4. **Reliability**: Graceful degradation, proper error handling
5. **Security**: Safe credential storage, no accidental leaks

## Future Enhancements

1. **Chat Mode**: Interactive conversation with context retention
2. **Template System**: Reusable prompts for common queries
3. **Cost Tracking**: Monitor API usage and costs
4. **Fine-tuning Support**: Use custom models
5. **Multi-modal**: Support for image/file analysis

## Migration from v0.2.x

If users have existing v0.2.x configurations:
1. Auto-detect old config format
2. Offer migration wizard
3. Preserve API keys and preferences
4. Update to new model names

## Timeline

- Week 1: Provider abstraction and configuration (TASK-020)
- Week 2: Raw LLM command implementation (TASK-021)
- Week 3: Ask command and context integration (TASK-018)
- Week 4: Testing and documentation
- Future: Agent mode (TASK-019)